# Configuration file for the Spark ML Training Pipeline

# --- ACTIVE STORAGE SELECTION ---
active_storage: alluxio

# --- STORAGE CONFIGURATIONS ---
data_source:
  # Alluxio master address
  base_path: "alluxio://master:19998/usr/ubuntu/data/classweights-43-21"
  train_path: "{base_path}/mini_train_df.parquet"
  test_path: "{base_path}/test_df.parquet"
  label_indexer_path: "{base_path}/data/label_indexer/label_indexer"
  temp_dir: "alluxio://master:19998/tmp"

# --- Artifact Storage Configuration (Where to WRITE final results) ---
artifact_storage:
  output_dir: "hdfs://master:9000/usr/ubuntu/results-ultra"

# General project settings
project_name: Multi_Model_NIDS_Training

# Training process settings
training:
  model_type: gru
  epochs: 1
  batch_size: 1024
  num_processes: 1  # Number of parallel processes for data loading | Equal to num_executors
  learning_rate: 0.01

# Spark configuration
spark:
  master: yarn
  deploy_mode: cluster
  executor_memory: 32G
  executor_cores: 4 
  num_executors: 12
  executor_memory_overhead: 4G
  driver_memory: 4G
  python_env: /home/ubuntu/spark_env/bin/python

spark-cluster2:
  master: yarn
  deploy_mode: cluster
  executor_memory: 50G
  executor_cores: 8
  num_executors: 6
  executor_memory_overhead: 4G
  driver_memory: 15G
  python_env: /home/ubuntu/spark_env/bin/python

spark-cluster3:
  master: yarn
  deploy_mode: cluster
  executor_memory: 20G
  executor_cores: 3
  num_executors: 12
  executor_memory_overhead: 3G
  driver_memory: 8G
  python_env: /home/ubuntu/spark_env/bin/python

spark-client:
  master: yarn
  deploy_mode: client
  executor_memory: 100G
  executor_cores: 8
  num_executors: 1
  executor_memory_overhead: 10G
  driver_memory: 30G
  python_env: /home/ubuntu/spark_env/bin/python

# Model architecture hyperparameters
model_params:
  default:
    hidden_size: 128
    num_layers: 3
    dropout: 0.2

  lstm:
    hidden_size: 256
    num_layers: 4
    dropout: 0.3

  gru:
    hidden_size: 128
    num_layers: 3
    dropout: 0.2

  rnn:
    hidden_size: 64
    num_layers: 2
    dropout: 0.15

  transformer:
    hidden_size: 256
    num_layers: 6
    dropout: 0.1
    num_heads: 8
    dim_feedforward: 512